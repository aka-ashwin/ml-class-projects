{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats.mstats import mode\n",
    "\n",
    "bag_filenum = 0\n",
    "boost_filenum = 0\n",
    "balearn_num = 0\n",
    "boolearn_num = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bagging loop\n",
    "#default tree depth = 1; uses stumps -- offers more direct comparison to stump-using AdaBoost\n",
    "#although deeper trees are probably ideal.\n",
    "def buildBagClassifier(X,y,K,treedepth = 1, counts_tiebreak = False):\n",
    "    #if tree depth is small (1 or 2), presort data, which will speed training in this case.\n",
    "    presort_data = (treedepth  < 3)\n",
    "    trees = []\n",
    "    \n",
    "    #tiebreaking stuff - ignore for now\n",
    "#     countdict = Counter(y.tolist())\n",
    "    \n",
    "#     n_output_vals = len(countdict.keys())\n",
    "#     if(not counts_tiebreak):\n",
    "#         tiebreak = dict(zip(countdict.keys(), np.ones(n_output_vals)))\n",
    "        \n",
    "    \n",
    "    for step in range(0,K):\n",
    "        newtree = DecisionTreeClassifier(max_depth = treedepth, presort = presort_data, splitter = 'random')\n",
    "        newtree.fit(X,y)\n",
    "        trees.append(newtree)\n",
    "        \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_ind(indices, inlist):\n",
    "    bestind = 0\n",
    "    bestval = inlist[indices[0]]\n",
    "    for i in indices:\n",
    "        if(inlist[i] > bestval):\n",
    "            bestval = inlist[i]\n",
    "            bestind = i\n",
    "    return bestind\n",
    "        \n",
    "def get_mode(inlist,breakers=None):\n",
    "#     counts = Counter(inlist)\n",
    "#note - doesn't deal well with multiple modes - would like to chose randomly (or on priors), but chooses arbitrarily \n",
    "#(deterministically)\n",
    "    outmode = mode(inlist)[0][0]\n",
    "    if(isinstance(outmode, np.ma.MaskedArray)):\n",
    "        outmode = outmode[0]\n",
    "    else: \n",
    "        #otherwise, should be array... I hope.\n",
    "        outmode = outmode\n",
    "    return outmode\n",
    "\n",
    "def bag_predict(x,bag,breakers=None):\n",
    "    preds = []\n",
    "    for tree in bag:\n",
    "        predcurr = tree.predict(x)\n",
    "        preds.append(predcurr)\n",
    "    return get_mode(preds,breakers)\n",
    "    \n",
    "def bag_score(xs,ys,bag,breakers=None):\n",
    "    correct = 0.\n",
    "    count = len(ys)\n",
    "    for i in range(0,count):\n",
    "        pred = bag_predict(xs[i].reshape(1,-1),bag,breakers)\n",
    "        if(pred == ys[i]):\n",
    "            correct += 1.\n",
    "    return correct/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#AdaBoost loop\n",
    "def buildAdaBoostClassifier(X,y,K):\n",
    "    nsamps = len(y)\n",
    "    \n",
    "    stumps = []\n",
    "    z = []\n",
    "    \n",
    "    weights = np.ones(nsamps)\n",
    "    weights = weights/nsamps\n",
    "    \n",
    "    #build K stumps with updating weights\n",
    "    for step in range(0,K):\n",
    "        #fit stump based on weights\n",
    "        #set presorting true to improve training speed\n",
    "        newstump = DecisionTreeClassifier(max_depth = 1, presort = True)\n",
    "        newstump.fit(X,y,sample_weight=weights)\n",
    "        \n",
    "        error = 0.\n",
    "        #if ever get no error, assume error would occur on average once in twice as large dataset\n",
    "        errmin = .5/nsamps\n",
    "        correct = []\n",
    "        \n",
    "        #sum (weighted) error; identify correctly-classified samples\n",
    "        for i in range(0,nsamps):\n",
    "            pred = newstump.predict(X[i].reshape(1,-1))\n",
    "            if(pred != y[i]):\n",
    "                error += weights[i]\n",
    "            else:\n",
    "                correct.append(i)\n",
    "        \n",
    "        if(error == 0.):\n",
    "            error = errmin\n",
    "        #reduce weight of correctly classified samples (based on quality of stump)\n",
    "        for i in correct:\n",
    "            weights[i] *= error/(1.-error)\n",
    "        \n",
    "        #normalize weights\n",
    "        weights = weights/sum(weights)\n",
    "        \n",
    "        #calculate stump-weight z\n",
    "        currz = np.log((1.-error)/error)\n",
    "        \n",
    "        #add weight z and stump newstump to appropriate lists\n",
    "        z.append(currz)\n",
    "        stumps.append(newstump)\n",
    "        \n",
    "    return stumps, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_max_key(indict):\n",
    "    keys = indict.keys()\n",
    "    max_key = None\n",
    "    max_val = None\n",
    "    first = True\n",
    "    for key in keys:\n",
    "        if(first):\n",
    "            max_val = indict[key]\n",
    "            max_key = key\n",
    "            first = False\n",
    "        elif(indict[key] > max_val):\n",
    "            max_val = indict[key]\n",
    "            max_key = key\n",
    "    return max_key\n",
    "    \n",
    "    \n",
    "def boost_predict(x,stumps,z):\n",
    "    scores = {}\n",
    "    for i in range(0,len(stumps)):\n",
    "        cpred = stumps[i].predict(x)\n",
    "        \n",
    "        #if guesses are arrays, assume they are trivial\n",
    "        #i.e. of form [[guess]], containing the single guess value (default behavior for numerical classes)\n",
    "        if(isinstance(cpred,np.ndarray)):\n",
    "            guess = cpred[0]\n",
    "        #otherwise, assume guesses are good keys\n",
    "        else:\n",
    "            guess = cpred\n",
    "            \n",
    "        try:\n",
    "            scores[guess] += z[i]\n",
    "        except:\n",
    "            scores[guess] = z[i]\n",
    "    return get_max_key(scores)\n",
    "            \n",
    "def boost_score(xs,ys,stumps, z):\n",
    "    correct = 0.\n",
    "    count = len(ys)\n",
    "    for i in range(0,count):\n",
    "        pred = boost_predict(xs[i].reshape(1,-1),stumps,z)\n",
    "        if(pred == ys[i]):\n",
    "            correct += 1\n",
    "    return correct/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_names = []\n",
    "for i in range(0,279):\n",
    "    arr_names.append(\"Input_\" + str(i))\n",
    "    \n",
    "arr_names.append(\"Output\")\n",
    "\n",
    "dfa = pd.read_table(\"../HW4/arrhythmia.data\", sep = \",\", header = None, names=arr_names, na_values = [\"?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "#impute mean for integer, float; impute mode otherwise\n",
    "\n",
    "\n",
    "def categorical_to_int(cat_col):\n",
    "    int_col = []\n",
    "    catmap = {}\n",
    "    currind = 0\n",
    "    for val in cat_col:\n",
    "        if val not in catmap.keys():\n",
    "            catmap[val] = currind\n",
    "            int_col.append(currind)\n",
    "            currind += 1\n",
    "        else:\n",
    "            int_col.append(catmap[val])\n",
    "    return int_col\n",
    "\n",
    "def imputed_version(dfin, excl_cols = []):\n",
    "    df = dfin.copy()\n",
    "    \n",
    "    copyimputed = False\n",
    "    imp_mean = Imputer(missing_values='NaN', strategy='mean', axis=0,copy=copyimputed)\n",
    "    imp_mode = Imputer(missing_values='NaN', strategy='most_frequent', axis=0, copy=copyimputed )\n",
    "\n",
    "    categorical_vars = []\n",
    "    #will use this list to make dummies for all categorical variables\n",
    "\n",
    "    for col in df.columns:\n",
    "        if(col in excl_cols):\n",
    "            continue\n",
    "        elif(df[col].dtype == np.float64 or  df[col].dtype == np.int64):\n",
    "            df[col] = imp_mean.fit_transform(df[col].reshape(-1,1))    \n",
    "        else:\n",
    "            categorical_vars.append(col)\n",
    "            #need to convert to integer column for Imputer to work\n",
    "            df[col] = categorical_to_int(df[col])\n",
    "            df[col] = imp_mode.fit_transform(df[col].reshape(-1,1))  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfa = imputed_version(dfa, ['Output'])\n",
    "# print(dfa['Input_13'][3])\n",
    "# print(np.mean(dfa['Input_13'][:]))\n",
    "\n",
    "Xa = np.asarray(dfa[arr_names[:279]])\n",
    "ya = np.asarray(dfa['Output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 279)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ya2 = []\n",
    "thresh = 4\n",
    "for i in range(0,len(ya)):\n",
    "    if(ya[i] <= thresh):\n",
    "        ya2.append(1)\n",
    "    else:\n",
    "        ya2.append(0)\n",
    "ya2 = np.asarray(ya2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xatrain = Xa[:][:350] \n",
    "testa = Xa[:][350].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yatrain = ya2[:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7771428571428571\n"
     ]
    }
   ],
   "source": [
    "treea = 151\n",
    "baga = buildBagClassifier(Xatrain,yatrain,treea)\n",
    "train_bag_a = bag_score(Xatrain, yatrain, baga)\n",
    "print(train_bag_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9685714285714285\n"
     ]
    }
   ],
   "source": [
    "boosta, za = buildAdaBoostClassifier(Xatrain,yatrain,treea)\n",
    "train_boost_a = boost_score(Xatrain, yatrain, boosta, za)\n",
    "print(train_boost_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525 done!\n",
      "551 done!\n",
      "575 done!\n",
      "601 done!\n"
     ]
    }
   ],
   "source": [
    "#Calculate validation curve values (training + test scores) on number K of learners\n",
    "\n",
    "# old_ntrees = [1,3,5,11,31,51,75,101,151,201,251] + [125,175,275,301,325]\n",
    "ntrees = [525,551,575,601]\n",
    "\n",
    "bags = False\n",
    "boost = True\n",
    "\n",
    "addto = True\n",
    "\n",
    "if(bags and not(addto)): \n",
    "    bag_trainscores = []\n",
    "    bag_testscores = []\n",
    "    ntrees_graph_bags = ntrees\n",
    "\n",
    "elif(bags and addto):\n",
    "    ntrees_graph_bags += ntrees\n",
    "    \n",
    "if(boost and not(addto)):\n",
    "    boost_trainscores = []\n",
    "    boost_testscores = []\n",
    "    ntrees_graph_boost = ntrees\n",
    "    \n",
    "elif(boost and addto):\n",
    "    ntrees_graph_boost += ntrees\n",
    "\n",
    "for treea in ntrees:\n",
    "    if(bags):\n",
    "        baga = buildBagClassifier(Xatrain,yatrain,treea,treedepth = bagdepth)\n",
    "        bag_trainscores.append(bag_score(Xatrain, yatrain, baga))\n",
    "        bag_testscores.append(bag_score(Xa[:][350:],ya2[350:],baga))\n",
    "    \n",
    "    if(boost):\n",
    "        boosta, za = buildAdaBoostClassifier(Xatrain,yatrain,treea)\n",
    "        boost_trainscores.append(boost_score(Xatrain, yatrain, boosta, za))\n",
    "        boost_testscores.append(boost_score(Xa[:][350:],ya2[350:],boosta,za))\n",
    "        \n",
    "    print(str(treea) + \" done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot stump bag K validation curve\n",
    "bag_filenum += 1\n",
    "plt.clf()\n",
    "# ax = plt.axes()\n",
    "plt.title(\"Bag Validation Curve\")\n",
    "plt.plot(ntrees_graph_bags,bag_trainscores,'b.',label = 'Bagging Training Scores')\n",
    "plt.plot(ntrees_graph_bags,bag_testscores,'r.', label = 'Bagging Test Scores')\n",
    "plt.xlabel(\"Number of Trees of Depth \" + str(bagdepth) + \" Used\")\n",
    "plt.ylabel(\"Score (Fraction Correct)\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "# plt.savefig('./7AdaBoost_and_StumpBag/bag_' + str(bag_filenum) + \"_depth_\" + str(bagdepth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot AdaBoost K validation curve\n",
    "boost_filenum += 1\n",
    "plt.clf()\n",
    "plt.title(\"AdaBoost Validation Curve\")\n",
    "plt.plot(ntrees_graph_boost,boost_trainscores,'b.', label = \"AdaBoost Training Scores\")\n",
    "plt.plot(ntrees_graph_boost,boost_testscores,'r.', label = \"AdaBoost Test Scores\")\n",
    "plt.xlabel(\"Number of Stumps Used\")\n",
    "plt.ylabel(\"Score (Fraction Correct)\")\n",
    "plt.legend(loc=4)\n",
    "plt.savefig('./7AdaBoost_and_StumpBag/boost_' + str(boost_filenum))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Learning curves: scores on test set vs size of train set \n",
    "#(note final curves won't include train set scores - just trying to replicate the Russell + Norvig graph here)\n",
    "\n",
    "#get AdaBoost test scores\n",
    "Kstar = 101\n",
    "trainsizes = range(10,360,35)\n",
    "boost_learnscores = []\n",
    "\n",
    "for ntrain in trainsizes:\n",
    "    currXa = Xatrain[:][:ntrain]\n",
    "    currya = yatrain[:ntrain]\n",
    "    boostal, zal = buildAdaBoostClassifier(currXa,currya,Kstar)\n",
    "    boost_learnscores.append(boost_score(Xa[:][360:],ya2[360:],boostal,zal))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Kbag = 101\n",
    "\n",
    "#get bag of stumps test scores\n",
    "bag_learnscores = []\n",
    "for ntrain in trainsizes:\n",
    "    currXa = Xatrain[:][:ntrain]\n",
    "    currya = yatrain[:ntrain]\n",
    "    bagal = buildBagClassifier(currXa,currya,Kstar)\n",
    "    bag_learnscores.append(bag_score(Xa[:][360:],ya2[360:],bagal))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "boolearn_num += 1\n",
    "plt.clf()\n",
    "plt.title(\"AdaBoost Learning Curve\")\n",
    "plt.plot(trainsizes,boost_learnscores,'r-', label = \"AdaBoost Learning Scores\")\n",
    "plt.plot(trainsizes, bag_learnscores, 'k--', label = \"Bagging Learning Scores\")\n",
    "plt.xlabel(\"Number of Training Examples\")\n",
    "plt.ylabel(\"Score (Fraction Correct) On Test Set\")\n",
    "plt.legend(loc=4)\n",
    "plt.savefig('./7AdaBoost_and_StumpBag/boost_learn_' + str(boolearn_num))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 279)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
