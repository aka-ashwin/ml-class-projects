{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3a \n",
    "#using nrows rows (10000 for final analysis, 1000 while setting up)\n",
    "nrows = 10000\n",
    "# df_old = pd.read_csv(\"train.csv\")[:nrows]\n",
    "df = pd.read_csv(\"train.csv\")[:nrows]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3c imputation\n",
    "\n",
    "#It could be useful to have a variable indicating whether the variable X_f had been imputed for a given row,\n",
    "#depending on the underlying data. For example, it might be the case that missing values for a survey question about income\n",
    "#are more likely for respondents with lower incomes--the missing values represent some significant bias, not just random loss.\n",
    "#however, if it is fairly certain that missing values are mostly due to e.g. data-entry errors, or are relatively few,\n",
    "#then adding an imputation indicator may be either useless or even somewhat harmful, as it could contribute to ovefitting.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "#impute mean for integer\n",
    "copyimputed = False\n",
    "imp_mean = Imputer(missing_values='NaN', strategy='mean', axis=0,copy=copyimputed)\n",
    "imp_mode = Imputer(missing_values='NaN', strategy='most_frequent', axis=0, copy=copyimputed )\n",
    "\n",
    "categorical_vars = []\n",
    "#will use this list to make dummies for all categorical variables\n",
    "\n",
    "def categorical_to_int(cat_col):\n",
    "    int_col = []\n",
    "    catmap = {}\n",
    "    currind = 0\n",
    "    for val in cat_col:\n",
    "        if val not in catmap.keys():\n",
    "            catmap[val] = currind\n",
    "            int_col.append(currind)\n",
    "            currind += 1\n",
    "        else:\n",
    "            int_col.append(catmap[val])\n",
    "    return int_col\n",
    "\n",
    "\n",
    "for col in df.columns:\n",
    "    if(df[col].dtype == np.float64 or  df[col].dtype == np.int64):\n",
    "        df[col] = imp_mean.fit_transform(df[col].reshape(-1,1))    \n",
    "    else:\n",
    "        categorical_vars.append(col)\n",
    "        #need to convert to integer column for Imputer to work\n",
    "        df[col] = categorical_to_int(df[col])\n",
    "        df[col] = imp_mode.fit_transform(df[col].reshape(-1,1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    3.0\n",
       "4    4.0\n",
       "5    4.0\n",
       "6    5.0\n",
       "7    4.0\n",
       "8    0.0\n",
       "9    2.0\n",
       "Name: Product_Info_2, dtype: float64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks / ways to get an understanding of the data\n",
    "# print(df['Product_Info_4'].dtype == np.float64)\n",
    "# df['Product_Info_2'][:10]\n",
    "# print(categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Product_Info_2: 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# dfcol = []\n",
    "# for col in df.columns[1:]:\n",
    "#     dfcol = df[col]\n",
    "\n",
    "#     if(dfcol.dtype == np.int64):\n",
    "#         print(col)\n",
    "#         print(Counter(dfcol.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product_Info_2']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Product_Info_1', 'Product_Info_3', 'Product_Info_4',\n",
      "       'Product_Info_5', 'Product_Info_6', 'Product_Info_7', 'Ins_Age', 'Ht',\n",
      "       'Wt',\n",
      "       ...\n",
      "       'Product_Info_2_B2', 'Product_Info_2_C1', 'Product_Info_2_C2',\n",
      "       'Product_Info_2_C3', 'Product_Info_2_C4', 'Product_Info_2_D1',\n",
      "       'Product_Info_2_D2', 'Product_Info_2_D3', 'Product_Info_2_D4',\n",
      "       'Product_Info_2_E1'],\n",
      "      dtype='object', length=146)\n"
     ]
    }
   ],
   "source": [
    "#3d \n",
    "\n",
    "#If all dummy variables are used, the input matrix X will not be full-rank, \n",
    "# as any dummy variable column can be constructed as a linear combination of the others,\n",
    "#   the definition of indicator variables leads to the simple element-level formula x^(i)_j = 1 - sum_{l \\neq i| (x^(l)_j) \n",
    "#   which implies that on a column level, x^(i) = [1,1,1,...,1] - sum_{l \\neq i} x^(l)\n",
    "\n",
    "\n",
    "\n",
    "for catvar in categorical_vars:\n",
    "    #create dataframe of dummy vars\n",
    "    df_toadd = pd.get_dummies(df[catvar], prefix = catvar)\n",
    "    #add to dataframe\n",
    "    df = df.join(df_toadd)\n",
    "    #remove source variable\n",
    "    df = df.ix[:, df.columns != catvar]\n",
    "\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3e\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.learning_curve import validation_curve\n",
    "from quadratic_kappa_for_notebook import qwk\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "#convert qwk to a scorer\n",
    "#idea from the student's answer for the piazza question at https://piazza.com/class/imcf964oqbo6qm?cid=74\n",
    "my_scorer = make_scorer(qwk, greater_is_better=True) \n",
    "\n",
    "X = np.asarray(df.ix[:, df.columns != 'Response'])\n",
    "# ensure no NaNs left\n",
    "# print(X[:3])\n",
    "# for col in X:\n",
    "#     for val in col:\n",
    "#         if np.isnan(val):\n",
    "#             print(\"na in :\" + str(col) + \", \" + str(val))\n",
    "\n",
    "y = df['Response'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Crange_init = [.1,.3,1.,10.,100.,1000.]\n",
    "#used with 1000-row dataset\n",
    "#yielded test scores of [0.9392, 0.9430, 0.9431, 0.9449, 0.9426, 0.9419]\n",
    "#best value = 10, but 1 and 100 are reasonably similar\n",
    "#will try to optimize a bit by looking more in this range\n",
    "\n",
    "Crange = [.1,.3,1.,3.,7.,10.,20.,30.,100.,300.,1000.]\n",
    "#with 10k-row dataset, best value was 20 with qwk = .9481\n",
    "\n",
    "reg = LogisticRegression()\n",
    "train_scores, test_scores = validation_curve(reg, X, y, \"C\", Crange, scoring = my_scorer, cv =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94599755154788467, 0.94830891819096741, 0.94899781670115213, 0.94953913886064834, 0.94871154543722425, 0.94854154608625585, 0.95104928652677612, 0.94934549318112249, 0.94868273583430318, 0.94679285611639796, 0.94916156478866021]\n",
      "[0.9440142399068705, 0.94654604007059517, 0.94732834721034853, 0.94706143370129747, 0.94644707114959981, 0.94676909848469604, 0.94812324041873774, 0.94717162589745207, 0.9460350619815564, 0.9443838174894521, 0.94619717848141449]\n"
     ]
    }
   ],
   "source": [
    "train_scores_avg = []\n",
    "test_scores_avg = []\n",
    "for i in range(0,len(train_scores)):\n",
    "    train_scores_avg.append(np.mean(train_scores[i]))\n",
    "    test_scores_avg.append(np.mean(test_scores[i]))\n",
    "print(train_scores_avg)\n",
    "print(test_scores_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(Crange), train_scores_avg,'ro')\n",
    "plt.plot(np.log(Crange),test_scores_avg,'go')\n",
    "plt.xlabel('ln(C)')\n",
    "plt.ylabel('Quadratic-Weighted Kappa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3f\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# l1_ratios = [.0003,.001,.01,.1]\n",
    "# alphas = [.003,.01,.1,10.]\n",
    "#best pre .003 was .1,.01, with score .946\n",
    "#best post was .1,.003, with .948\n",
    "\n",
    "# l1_ratios = [.1,.3,.5,.9]\n",
    "# alphas = [.001,.003,.01]\n",
    "#best was .5,.001, with score .9484\n",
    "\n",
    "l1_ratios = [.4,.5,.6]\n",
    "alphas = [.0001,.0003,.001,.003]\n",
    "#best was .5,.001, with ~the same score\n",
    "\n",
    "search_grid = {'l1_ratio':l1_ratios,'alpha':alphas}\n",
    "\n",
    "ent = ElasticNet(max_iter = 2000)\n",
    "gscv = GridSearchCV(ent,search_grid,cv=5, scoring=my_scorer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.0001, copy_X=True, fit_intercept=True, l1_ratio=0.6,\n",
      "      max_iter=2000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "0.954058271398\n"
     ]
    }
   ],
   "source": [
    "gscv.fit(X,y)\n",
    "print(gscv.best_estimator_)\n",
    "print(gscv.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3g\n",
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "#logistic regression is solidly superior by our metric; we will use it with the optimal C-value of 20.\n",
    "\n",
    "regstar = LogisticRegression(C=20.)\n",
    "\n",
    "ltrain_sizes = np.array([ 0.1, 0.33, 0.55, 0.78, 1. ])\n",
    "sizesout, train_scores_learn, test_scores_learn = learning_curve(regstar, X, y, cv = 5, scoring = my_scorer, train_sizes = ltrain_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97161527167848294, 0.95756455116862027, 0.95341656799073937, 0.95131644914058955, 0.94831242837314134]\n",
      "[0.91516505551867433, 0.94902848163115439, 0.94973656723672373, 0.94889343772327917, 0.94679872312284663]\n"
     ]
    }
   ],
   "source": [
    "ltrain_scores_avg = []\n",
    "ltest_scores_avg = []\n",
    "for i in range(0,len(train_scores_learn)):\n",
    "    ltrain_scores_avg.append(np.mean(train_scores_learn[i]))\n",
    "    ltest_scores_avg.append(np.mean(test_scores_learn[i]))\n",
    "print(ltrain_scores_avg)\n",
    "print(ltest_scores_avg)\n",
    "               \n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(ltrain_sizes, ltrain_scores_avg,'ro')\n",
    "plt.plot(ltrain_sizes,ltest_scores_avg,'go')\n",
    "plt.xlabel('Fraction of Data Used')\n",
    "plt.ylabel('Quadratic-Weighted Kappa')\n",
    "plt.title('Learning Curve for Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The training and test qwk curves become very close when the full 10,000 rows are used. \n",
    "#This implies that there is very little error due to variance (dependency of the fitted model on the data set chosen); \n",
    "# the error is almost entirely due to bias (inability of the model to capture all the attributes of the system)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
